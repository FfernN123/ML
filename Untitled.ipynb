{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the agent will be painted by gray 0.8\n",
    "agent_mark = 0.5      # The current agent cell will be painteg by gray 0.5\n",
    "flag_mark = 0.25\n",
    "\n",
    "\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "# actions_dict = {\n",
    "#     0: 'left',\n",
    "#     1: 'up',\n",
    "#     2: 'right',\n",
    "#     3: 'down',\n",
    "# }\n",
    "\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze,flags, agent=(0,0), target=None):\n",
    "        self._maze = np.array(maze)\n",
    "        self._flags = set(flags)\n",
    "        self.states = list()\n",
    "        self.states.append((0,0))\n",
    "        nrows, ncols = self._maze.shape\n",
    "        if target is None:\n",
    "            self.target = (nrows-1, ncols-1)   # target cell where the \"Destination\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not agent in self.free_cells:\n",
    "            raise Exception(\"Invalid Agent Location: must sit on a free cell\")\n",
    "        self.reset(agent)\n",
    "\n",
    "    def reset(self, agent):\n",
    "        self.agent = agent\n",
    "        self.maze = np.copy(self._maze)\n",
    "        self.flags = set(self._flags)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = agent\n",
    "        self.maze[row, col] = agent_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = dict(((r,c),0) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0)\n",
    "#         self.visited = set()\n",
    "        self.base = np.sqrt(self.maze.size)\n",
    "        self.reward = {\n",
    "            'blocked':  self.min_reward,\n",
    "            'flag':     1.0/len(self._flags),\n",
    "            'invalid': -4.0/self.base,\n",
    "            'valid':   -1.0/self.maze.size\n",
    "        }\n",
    "        \n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = agent_row, agent_col, mode = self.state\n",
    "        agent = (agent_row, agent_col)\n",
    "        \n",
    "        if self.maze[agent] > 0.0:\n",
    "            self.visited[agent] += 1  # mark visited cell\n",
    "#         print(self.visited)\n",
    "        if agent in self.flags:\n",
    "            self.flags.remove(agent)\n",
    "            print(self.flags)\n",
    "\n",
    "        \n",
    "        valid_actions = self.valid_actions() \n",
    "        \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in agent position\n",
    "            mode = 'invalid'\n",
    "            self.states.pop(-1)\n",
    "\n",
    "        s = (nrow,ncol)\n",
    "        self.states.append(s)   \n",
    "        \n",
    "        # new state\n",
    "        state = self.state = (nrow, ncol, nmode)\n",
    "        return self.states\n",
    "\n",
    "    def get_reward(self):\n",
    "        agent_row, agent_col, mode = self.state\n",
    "        agent = (agent_row,agent_col)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if agent == self.target:\n",
    "            return 1.0 - len(self.flags) / len(self._flags)\n",
    "        if mode == 'blocked':\n",
    "            return self.reward['blocked']\n",
    "        if agent in self.flags:\n",
    "            return self.reward['flag']\n",
    "        if mode == 'invalid':\n",
    "            return self.reward['invalid']\n",
    "        if mode == 'valid':\n",
    "            return self.reward['valid']\n",
    "\n",
    "    def act(self, action):\n",
    "        states = self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status, states, self.free_cells\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        \n",
    "        # draw the flags\n",
    "        for r,c in self.flags:\n",
    "            canvas[r,c] = flag_mark\n",
    "            \n",
    "        # draw the agent\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = agent_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        \n",
    "        agent_row, agent_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        \n",
    "        if agent_row == nrows-1 and agent_col == ncols-1: \n",
    "            return 'win' #when go to destination\n",
    "        \n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.states:\n",
    "        canvas[row,col] = visited_mark\n",
    "    for row,col in qmaze.flags:\n",
    "        canvas[row,col] = flag_mark\n",
    "    agent_row, agent_col, _ = qmaze.state\n",
    "    canvas[agent_row, agent_col] = agent_mark   # agent cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='Greens_r')\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  0.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "])\n",
    "\n",
    "flags = [(3,0), (5,2), (1,5), (3,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= 0.25\n",
      "states: [(0, 0), (1, 0), (2, 0), (2, 1), (2, 0), (3, 0)]\n",
      "==============================env_state===================================\n",
      "[[1.   0.   1.   1.   1.   1.   1.   1.   0.   1.   0.   0.   0.25 1.\n",
      "  1.   1.   1.   0.   0.   1.   1.   0.5  1.   1.   0.   0.   1.   0.25\n",
      "  1.   1.   1.   0.   0.   1.   1.   1.   1.   0.25 0.   0.   1.   1.\n",
      "  1.   1.   1.   1.   1.   1.   1.  ]]\n",
      "==========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x225a1275fd0>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = Qmaze(maze,flags)\n",
    "canvas, reward, game_over, states,f = qmaze.act(DOWN)\n",
    "canvas, reward, game_over, states,f = qmaze.act(RIGHT) \n",
    "canvas, reward, game_over, states,f = qmaze.act(RIGHT)  \n",
    "canvas, reward, game_over, states,f = qmaze.act(RIGHT)  \n",
    "canvas, reward, game_over, states,f = qmaze.act(DOWN)\n",
    "canvas, reward, game_over, states,f = qmaze.act(RIGHT)\n",
    "canvas, reward, game_over, states,f = qmaze.act(LEFT)\n",
    "canvas, reward ,game_over, states,f = qmaze.act(DOWN)\n",
    "\n",
    "print(\"reward=\", reward)\n",
    "print(\"states:\",states)\n",
    "print(\"==============================env_state===================================\")\n",
    "print(canvas)\n",
    "print(\"==========================================================================\")\n",
    "\n",
    "# print(f)\n",
    "\n",
    "show(qmaze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, agent_cell):\n",
    "    qmaze.reset(agent_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, status, states, free_cells = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the class in which we collect our game episodes (or game experiences) within a memory list.\n",
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1] #4\n",
    "    \n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0] #delete the oldest episode\n",
    "\n",
    "    def predict(self, envstate):\n",
    "#         print(self.model.predict(envstate)) #[[ 0.10858168 -0.6033055   0.08228942 -0.5122358 ]]\n",
    "#         print(self.model.predict(envstate)[0]) #[ 0.10858168 -0.6033055   0.08228942 -0.5122358 ]\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
